# -*- coding: utf-8 -*-
"""Flight_Price_Pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TSK0LNig8iS1n5FsseE7g_LKWW6VKvs9
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score

np.random.seed(1)

# Load the cleaned dataset
df = pd.read_csv('Clean_Dataset.csv')

# Drop rows with missing values (if any)
df = df.dropna()

# Prepare the DataFrame for model training
df_prep = df.drop(['Unnamed: 0', 'flight'], axis=1)

# Convert 'class' to binary encoding (assuming 'Business' and 'Economy')
df_prep['class'] = df_prep['class'].apply(lambda x: 1 if x == 'Business' else 0)

# Convert 'stops' to numerical values (assuming 'zero', 'one', 'two' stops)
df_prep['stops'] = df_prep['stops'].apply(lambda x: 0 if x == 'zero' else 1 if x == 'one' else 2)

# Perform one-hot encoding for categorical variables
df_prep = pd.get_dummies(df_prep, drop_first=True)

# Separate features and target variable
X = df_prep.drop('price', axis=1)  # Features
y = df_prep['price']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize various regression models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'Polynomial Regression': make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),  # Example of Polynomial Regression with degree 2
    'Decision Tree Regression': DecisionTreeRegressor(),
    'Random Forest Regression': RandomForestRegressor(),
    'Gradient Boosting Regression': GradientBoostingRegressor()
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results[name] = {'Mean Absolute Error': mae, 'R2 Score': r2}

# Print results
for name, result in results.items():
    print(f"Model: {name}")
    print(f"  Mean Absolute Error: {result['Mean Absolute Error']}")
    print(f"  R2 Score: {result['R2 Score']}")
    print()

#Random Forest Regression gives the best results
#followed by Decision Tree Regression,
#Gradient Boosting Regression,
#Polynomial Regression,
#Linear Regression, Ridge Regression, Lasso Regression.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score

np.random.seed(1)

# Load the cleaned dataset
df = pd.read_csv('Clean_Dataset.csv')

# Drop rows with missing values (if any)
df = df.dropna()

# Prepare the DataFrame for model training
df_prep = df.drop(['Unnamed: 0', 'flight'], axis=1)

# Convert 'class' to binary encoding (assuming 'Business' and 'Economy')
df_prep['class'] = df_prep['class'].apply(lambda x: 1 if x == 'Business' else 0)

# Convert 'stops' to numerical values (assuming 'zero', 'one', 'two' stops)
df_prep['stops'] = df_prep['stops'].apply(lambda x: 0 if x == 'zero' else 1 if x == 'one' else 2)

# Perform one-hot encoding for categorical variables
df_prep = pd.get_dummies(df_prep, drop_first=True)

# Separate features and target variable
X = df_prep.drop('price', axis=1)  # Features
y = df_prep['price']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize various regression models
models = {
    'Decision Tree Regression': DecisionTreeRegressor(),
    'Random Forest Regression': RandomForestRegressor(),
    'Gradient Boosting Regression': GradientBoostingRegressor()
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results[name] = {'Mean Absolute Error': mae, 'R2 Score': r2}

# Print results
for name, result in results.items():
    print(f"Model: {name}")
    print(f"  Mean Absolute Error: {result['Mean Absolute Error']}")
    print(f"  R2 Score: {result['R2 Score']}")
    print()

#Random Forest Regression gives the best results
#followed by Decision Tree Regression,
#Gradient Boosting Regression,
#Polynomial Regression,
#Linear Regression, Ridge Regression, Lasso Regression.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

np.random.seed(1)

# Load the cleaned dataset
df = pd.read_csv('Clean_Dataset.csv')

# Drop rows with missing values (if any)
df = df.dropna()

# Prepare the DataFrame for model training
df_prep = df.drop(['Unnamed: 0', 'flight'], axis=1)

# Convert 'class' to binary encoding (assuming 'Business' and 'Economy')
df_prep['class'] = df_prep['class'].apply(lambda x: 1 if x == 'Business' else 0)

# Convert 'stops' to numerical values (assuming 'zero', 'one', 'two' stops)
df_prep['stops'] = df_prep['stops'].apply(lambda x: 0 if x == 'zero' else 1 if x == 'one' else 2)

# Perform one-hot encoding for categorical variables
df_prep = pd.get_dummies(df_prep, drop_first=True)

# Separate features and target variable
X = df_prep.drop('price', axis=1)  # Features
y = df_prep['price']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize Random Forest Regression model
model = RandomForestRegressor()

# Hyperparameter grid for Random Forest Regression
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20]
}

# Train and evaluate the model with hyperparameter tuning
print("Training Random Forest Regression...")
grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_absolute_error')
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
results = {
    'Best Params': grid_search.best_params_,
    'Mean Absolute Error': mae,
    'R2 Score': r2
}

# Print results
print("Model: Random Forest Regression")
print(f"  Best Params: {results['Best Params']}")
print(f"  Mean Absolute Error: {results['Mean Absolute Error']}")
print(f"  R2 Score: {results['R2 Score']}")
print()

